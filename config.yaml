mode: train #export #evaluate #serve #inference
training:
  batch_size: 64
  block_size: 128
  dropout: 0.2
  device: "cuda"
  use_mlflow: True
  epochs: 50
  log_interval: 5
  save_interval: 1
  output_dir: "./saved_model/"
  max_iters : 500
  eval_iters : 5


lora:
  rank: 16
  alpha: 32
  dropout: 0.1

optimizer:
  lr: 0.002
  weight_decay: 0.01

scheduler:
  step_size: 10
  gamma: 0.1


model:
  dropout: 0.2










