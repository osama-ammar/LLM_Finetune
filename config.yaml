mode: train #export #evaluate #serve #inference
training:
  batch_size: 64
  block_size: 128
  dropout: 0.2
  use_mlflow: True
  epochs: 6
  log_interval: 5
  save_interval: 1
  output_dir: "./saved_model/"
  max_iters : 500
  eval_iters : 5
  quatize: True
  use_lora_ft : True
  use_soft_prompt : False

lora:
  rank: 16
  alpha: 32
  dropout: 0.1

optimizer:
  lr: 0.002
  weight_decay: 0.01

scheduler:
  step_size: 10
  gamma: 0.1


model:
  dropout: 0.2










